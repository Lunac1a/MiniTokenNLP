{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "48cd4c06",
      "metadata": {
        "id": "48cd4c06"
      },
      "source": [
        "\n",
        "# Subword Tokenizer (BPE) + Mini Sentiment Demo\n",
        "**Task:** 构建一个简单的BPE Tokenizer，并用它完成一个小型情感分类任务\n",
        "\n",
        "**Step:**\n",
        "1. 在一个小Corpus 上训练 BPE Tokenizer\n",
        "2. 使用从Corpus里学习到的合并规则，对文本进行编码/解码\n",
        "3. 将 BPE 分词结果作为特征，用于情感分类。\n",
        "4. 简单评估并输出示例预测结果\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "677y_oC240TD",
      "metadata": {
        "id": "677y_oC240TD"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ecb50390",
      "metadata": {
        "id": "ecb50390"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Imports\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "import random\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "# ML bits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be2e61ba",
      "metadata": {
        "id": "be2e61ba"
      },
      "source": [
        "\n",
        "## 1) Tiny Training Corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "136b52ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "136b52ba",
        "outputId": "ed5f28b0-8a20-49d8-b0ce-6d9eef0091da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\n",
        "train_corpus = [\n",
        "    \"i love this movie it is fantastic\",\n",
        "    \"this film is terrible and boring\",\n",
        "    \"what a wonderful day for cinema\",\n",
        "    \"i dislike the ending of this movie\",\n",
        "    \"great acting and a lovely soundtrack\",\n",
        "    \"the plot is weak and predictable\",\n",
        "    \"absolutely amazing visuals and pacing\",\n",
        "    \"i will not recommend this film\",\n",
        "    \"such a charming story and vibe\",\n",
        "    \"the characters were flat and dull\",\n",
        "    \"i enjoyed every minute of it\",\n",
        "    \"waste of time and money\",\n",
        "    \"brilliant performances and direction\",\n",
        "    \"i hate how slow it becomes\",\n",
        "    \"heartwarming scenes and solid writing\",\n",
        "    \"the dialogue felt unnatural\",\n",
        "    \"an inspiring and delightful experience\",\n",
        "    \"poor editing ruined the tension\",\n",
        "    \"i like the humor and style\",\n",
        "    \"the soundtrack is annoying\",\n",
        "]\n",
        "len(train_corpus)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ede6fd1",
      "metadata": {
        "id": "7ede6fd1"
      },
      "source": [
        "\n",
        "## 2) Implement BPE\n",
        "使用经典的 基于单词的 BPE，并添加单词结束标记 </w>：\n",
        "1. 将每个单词表示为字符序列 + </w>\n",
        "2. 统计整个语料中字符对的出现频率\n",
        "3. 将出现频率最高的字符对合并为一个新符号\n",
        "4. 重复上述步骤，直到完成 N 次合并\n",
        "5. 分词时按照从高频到低频的顺序，贪心应用合并规则"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0264908f",
      "metadata": {
        "id": "0264908f"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "from typing import List, Tuple\n",
        "\n",
        "# 按空白字符分词，返回单词列表\n",
        "def whitespace_tokenize(text: str) -> List[str]:\n",
        "    return re.findall(r\"\\S+\", text.strip())  # \\S+ 匹配非空白字符序列\n",
        "\n",
        "# 准备 BPE 训练词表\n",
        "def prepare_bpe_training(corpus: List[str]) -> Counter:\n",
        "    # 将语料按行拆分为单词，并统一小写\n",
        "    words = []\n",
        "    for line in corpus:\n",
        "        words.extend(whitespace_tokenize(line.lower()))\n",
        "\n",
        "    # 将每个单词拆成字符 + 单词结束标记 '</w>'，并统计频率\n",
        "    vocab = Counter(tuple(list(w) + ['</w>']) for w in words)\n",
        "    return vocab\n",
        "\n",
        "# 统计词表中所有相邻字符对的频率\n",
        "def get_pair_stats(vocab: Counter) -> Counter:\n",
        "    pairs = Counter()\n",
        "    for symbols, freq in vocab.items():  # symbols 是字符元组\n",
        "        for i in range(len(symbols)-1):\n",
        "            pair = (symbols[i], symbols[i+1])  # 相邻字符对\n",
        "            pairs[pair] += freq\n",
        "    return pairs\n",
        "\n",
        "# 合并词表中指定的高频字符对\n",
        "def merge_vocab(vocab: Counter, pair: Tuple[str, str]) -> Counter:\n",
        "    # 使用正则将指定字符对合并为一个新符号\n",
        "    bigram = pair[0] + pair[1]  # 新符号\n",
        "    new_vocab = Counter()\n",
        "    for symbols, freq in vocab.items():\n",
        "        s = ' '.join(symbols)  # 将元组转换为空格分隔的字符串方便替换\n",
        "        # 将匹配的字符对替换为 bigram\n",
        "        s = s.replace(pair[0] + ' ' + pair[1], bigram)\n",
        "        # 再转回元组，并记录频率\n",
        "        new_vocab[tuple(s.split(' '))] += freq\n",
        "    return new_vocab\n",
        "\n",
        "# 从语料中学习 BPE 合并规则\n",
        "def learn_bpe(corpus: List[str], num_merges: int = 100):\n",
        "    vocab = prepare_bpe_training(corpus)  # 准备词表\n",
        "    merges = []  # 存储每一步的合并对\n",
        "    for _ in range(num_merges):\n",
        "        pairs = get_pair_stats(vocab)  # 统计所有字符对\n",
        "        if not pairs:  # 如果没有字符对可合并，结束\n",
        "            break\n",
        "        best = max(pairs, key=pairs.get)  # 选择出现频率最高的字符对\n",
        "        if pairs[best] < 2:  # 如果频率太低，也停止\n",
        "            break\n",
        "        vocab = merge_vocab(vocab, best)  # 合并选中的字符对\n",
        "        merges.append(best)  # 记录合并\n",
        "    return merges  # 返回所有学习到的合并规则\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "544bae3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "544bae3c",
        "outputId": "d8ca984d-514c-419d-e5cd-fde6be5995a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78,\n",
              " [('e', '</w>'),\n",
              "  ('a', 'n'),\n",
              "  ('d', '</w>'),\n",
              "  ('i', 'n'),\n",
              "  ('s', '</w>'),\n",
              "  ('t', 'h'),\n",
              "  ('an', 'd</w>'),\n",
              "  ('in', 'g'),\n",
              "  ('ing', '</w>'),\n",
              "  ('t', '</w>')])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# 从训练语料中学习 80 次字符合并（可以根据语料大小或演示时间调整次数）\n",
        "merges = learn_bpe(train_corpus, num_merges=80)\n",
        "\n",
        "# 查看学习到的合并规则数量和前 10 条规则\n",
        "len(merges), merges[:10]  # len(merges) 返回总共学习到的合并对数量\n",
        "                         # merges[:10] 返回最前面 10 个高频合并对\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "41f8e1a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41f8e1a4",
        "outputId": "9346b373-84cc-4d57-8403-f0f10c5784b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example: ['a', 'b', 'so', 'l', 'ut', 'ely</w>', 'lov', 'ely</w>', 'movi', 'e', '!']\n"
          ]
        }
      ],
      "source": [
        "# 定义 BPE 分词器类\n",
        "class BPETokenizer:\n",
        "    def __init__(self, merges):\n",
        "        # 将 merges 列表存储为字典 {字符对: 排名}，排名越小优先级越高\n",
        "        self.rank = {pair: i for i, pair in enumerate(merges)}\n",
        "\n",
        "    # 内部方法：对单个单词应用 BPE 贪心合并\n",
        "    def _bpe(self, word: str):\n",
        "        # 将单词拆成字符 + 单词结束标记 '</w>'\n",
        "        symbols = list(word) + ['</w>']\n",
        "\n",
        "        # 贪心合并：重复合并优先级最高的相邻字符对\n",
        "        while True:\n",
        "            if len(symbols) < 2:  # 如果少于 2 个符号，无法再合并\n",
        "                break\n",
        "            # 构建所有相邻字符对\n",
        "            pairs = [(symbols[i], symbols[i+1]) for i in range(len(symbols)-1)]\n",
        "            if not pairs:\n",
        "                break  # 避免空列表传给 min() 报错\n",
        "\n",
        "            # 为每个字符对查找排名，如果没有在 merges 中出现，则排名为 1e9（低优先级）\n",
        "            ranked = [(self.rank.get(p, 1e9), i, p) for i, p in enumerate(pairs)]\n",
        "            if all(r[0] == 1e9 for r in ranked):  # 如果没有可用的合并对，停止\n",
        "                break\n",
        "\n",
        "            # 选择排名最小（优先级最高）的字符对\n",
        "            best_rank, idx, best_pair = min(ranked, key=lambda x: x[0])\n",
        "            if best_rank == 1e9:\n",
        "                break  # 没有可用的合并对\n",
        "\n",
        "            # 在 idx 位置合并字符对\n",
        "            symbols = symbols[:idx] + [symbols[idx] + symbols[idx+1]] + symbols[idx+2:]\n",
        "\n",
        "        # 如果末尾还有 '</w>'，去掉它\n",
        "        if symbols and symbols[-1] == '</w>':\n",
        "            symbols = symbols[:-1]\n",
        "\n",
        "        return symbols  # 返回 BPE 分词结果列表\n",
        "\n",
        "    # 对文本进行 BPE 编码\n",
        "    def encode(self, text: str):\n",
        "        tokens = []\n",
        "        for word in whitespace_tokenize(text.lower()):  # 按空格分词并转小写\n",
        "            pieces = self._bpe(word)  # 对每个单词应用 BPE\n",
        "            tokens.extend(pieces)\n",
        "        return tokens\n",
        "\n",
        "    # 对 BPE token 列表进行简单解码\n",
        "    def decode(self, tokens):\n",
        "        return ' '.join(tokens)  # 直接用空格连接 token\n",
        "\n",
        "# 创建 BPE 分词器实例\n",
        "tokenizer = BPETokenizer(merges)\n",
        "\n",
        "# 测试示例\n",
        "print(\"Example:\", tokenizer.encode(\"Absolutely lovely movie!\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "451d4455",
      "metadata": {
        "id": "451d4455"
      },
      "source": [
        "其中，_bpe 是核心函数，对单词进行贪心合并，直到无法再合并为止\n",
        "\n",
        "encode 将文本按空格拆成单词，并对每个单词调用 _bpe，而 decode 只是演示用，将 token 列表简单拼接成字符串\n",
        "\n",
        "输出示例可以看到单词被分解成子词 token，例如 Absolutely 可能被分成 ['A', 'b', 'so', 'lutely']等等"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a493741",
      "metadata": {
        "id": "5a493741"
      },
      "source": [
        "\n",
        "### Quick sanity checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "947dedb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "947dedb4",
        "outputId": "15c1ad61-d1bc-4787-e2c6-99fde714a403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I loved the movie -> ['i</w>', 'lov', 'e', 'd</w>', 'the</w>', 'movie</w>']\n",
            "The plot was weak -> ['the</w>', 'p', 'lo', 't</w>', 'wa', 's</w>', 'w', 'ea', 'k</w>']\n",
            "Charming visuals and story -> ['cha', 'rming</w>', 'vi', 'su', 'a', 'l', 's</w>', 'and</w>', 'st', 'or', 'y</w>']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for s in [\"I loved the movie\", \"The plot was weak\", \"Charming visuals and story\"]:\n",
        "    print(s, \"->\", tokenizer.encode(s))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0aabc7d",
      "metadata": {
        "id": "a0aabc7d"
      },
      "source": [
        "\n",
        "## 3) Use BPE Tokens as Features for Sentiment\n",
        "接着在一个小数据集上训练一个简单的 多项式朴素贝叶斯 分类器，使用 BPE Token 作为情感特征\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7003fc75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7003fc75",
        "outputId": "ecbf9e01-9c43-4202-9c41-0c3277a9e049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3333333333333333\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.333     0.333     0.333         3\n",
            "           1      0.333     0.333     0.333         3\n",
            "\n",
            "    accuracy                          0.333         6\n",
            "   macro avg      0.333     0.333     0.333         6\n",
            "weighted avg      0.333     0.333     0.333         6\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#小型带标签数据集（正面=1，负面=0\n",
        "dataset = [\n",
        "    (\"i love this movie\", 1),\n",
        "    (\"this film is great\", 1),\n",
        "    (\"absolutely amazing visuals\", 1),\n",
        "    (\"what a wonderful story\", 1),\n",
        "    (\"i like the acting and music\", 1),\n",
        "    (\"heartwarming and inspiring\", 1),\n",
        "    (\"i enjoyed every minute\", 1),\n",
        "    (\"a delightful experience\", 1),\n",
        "\n",
        "    (\"this film is terrible\", 0),\n",
        "    (\"i hate the ending\", 0),\n",
        "    (\"boring and predictable plot\", 0),\n",
        "    (\"waste of time\", 0),\n",
        "    (\"annoying soundtrack\", 0),\n",
        "    (\"characters were flat\", 0),\n",
        "    (\"poor editing ruined it\", 0),\n",
        "    (\"dull and weak writing\", 0),\n",
        "]\n",
        "\n",
        "# 将文本和标签分别提取出来\n",
        "texts = [t for t,_ in dataset]   # 文本列表\n",
        "labels = [y for _,y in dataset]  # 标签列表\n",
        "\n",
        "#使用 CountVectorizer 结合自定义 BPE 分词器\n",
        "# 参数说明：\n",
        "# - lowercase=True: 文本转为小写\n",
        "# - tokenizer=tokenizer.encode: 使用之前定义的 BPE 分词器\n",
        "# - preprocessor=None: 不进行额外预处理\n",
        "vectorizer = CountVectorizer(lowercase=True, tokenizer=tokenizer.encode, preprocessor=None)\n",
        "\n",
        "# 构建词频矩阵\n",
        "X = vectorizer.fit_transform(texts)\n",
        "y = labels\n",
        "\n",
        "# 划分训练集和测试集\n",
        "# test_size=0.35: 35% 数据作为测试集\n",
        "# random_state=42: 保证可重复性\n",
        "# stratify=y: 按标签比例分层采样\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42, stratify=y)\n",
        "\n",
        "# 训练多项式朴素贝叶斯分类器\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 在测试集上进行预测\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 输出结果\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))  # 准确率\n",
        "print(classification_report(y_test, y_pred, digits=3))  # 精确率、召回率、F1 等详细指标\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c4ea56a",
      "metadata": {
        "id": "9c4ea56a"
      },
      "source": [
        "\n",
        "### Try own sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f827452e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f827452e",
        "outputId": "e40995dd-5ee3-4aca-bb5d-b2878abc51c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'i absolutely love this film' -> negative (p_pos=0.098)\n",
            "'the movie is dull and boring' -> negative (p_pos=0.024)\n",
            "'great pacing but weak ending' -> negative (p_pos=0.026)\n",
            "'not my type, but okay' -> positive (p_pos=0.862)\n"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "\n",
        "# 定义一个函数，用于对新文本进行情感预测\n",
        "def predict_sentiment(sentences: List[str]):\n",
        "    # 将新文本转换为 BPE token 特征矩阵\n",
        "    X_new = vectorizer.transform(sentences)\n",
        "\n",
        "    # 使用训练好的朴素贝叶斯分类器进行预测\n",
        "    preds = clf.predict(X_new)         # 预测类别（0=负面，1=正面）\n",
        "    probs = clf.predict_proba(X_new)[:,1]  # 获取正面情感概率\n",
        "\n",
        "    # 输出每条文本的预测结果和正面概率\n",
        "    for s, p, pr in zip(sentences, preds, probs):\n",
        "        sentiment = 'positive' if p == 1 else 'negative'\n",
        "        print(f\"{s!r} -> {sentiment} (p_pos={pr:.3f})\")\n",
        "\n",
        "# 测试预测函数\n",
        "predict_sentiment([\n",
        "    \"i absolutely love this film\",\n",
        "    \"the movie is dull and boring\",\n",
        "    \"great pacing but weak ending\",\n",
        "    \"not my type, but okay\",\n",
        "])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}