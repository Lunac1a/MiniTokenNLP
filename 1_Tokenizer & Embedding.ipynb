{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eafe6df879acbe3",
   "metadata": {},
   "source": [
    "# MiniTokenNLP â€“ Custom Tokenizer & Embedding Visualization\n",
    "\n",
    "**Goal:** Build a minimal tokenizer and embedding layer from scratch, visualize the resulting feature space, and interpret patterns in word representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bcdc9ea2b47e6b",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "This notebook demonstrates how text data can be numerically represented through tokenization and embedding.\n",
    "\n",
    "Steps:\n",
    "1. Load and clean movie review texts.\n",
    "2. Build a simple tokenizer from scratch.\n",
    "3. Construct random embeddings for each word.\n",
    "4. Visualize the embedding space.\n",
    "5. Discuss what these visualizations reveal about language representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f861d5cf84edf2",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation\n",
    "We use NLTK's `movie_reviews` dataset which contains 2000 labeled reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T05:54:23.000019Z",
     "start_time": "2025-10-13T05:54:22.319555Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append(\"./nltk_data\")\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "\n",
    "# Load dataset\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655acbf34f8d4156",
   "metadata": {},
   "source": [
    "## 3. Text Cleaning\n",
    "To make the vocabulary more meaningful, we:\n",
    "- Lowercase all words\n",
    "- Remove punctuation and stopwords\n",
    "- Keep alphabetic tokens only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b43bfc3891ccd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T05:55:07.951472Z",
     "start_time": "2025-10-13T05:55:07.841925Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(words):\n",
    "    return [w.lower() for w in words if w.isalpha() and w.lower() not in stop_words]\n",
    "\n",
    "cleaned_texts = [clean_text(words) for words, _ in documents]\n",
    "\n",
    "print(cleaned_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bdeb380369f71e",
   "metadata": {},
   "source": [
    "## 4. Custom Tokenizer\n",
    "We build a simple word-level tokenizer that constructs a vocabulary and assigns each word an integer ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1283890daecd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T05:57:36.053771Z",
     "start_time": "2025-10-13T05:57:35.984286Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Define a simple word-level tokenizer class\n",
    "class MiniTokenizer:\n",
    "    def __init__(self, min_freq=3):\n",
    "        \"\"\"\n",
    "        Initialize the tokenizer.\n",
    "        :arg min_freq(int): minimum frequency a word must have to be included in the vocabulary.\n",
    "        \"\"\"\n",
    "        self.min_freq = min_freq\n",
    "        # Initialize vocabulary with two special tokens:\n",
    "        # <PAD> for padding and <UNK> for unknown (out-of-vocabulary) words.\n",
    "        self.word2id = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.id2word = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "\n",
    "    def fit(self, texts):\n",
    "        \"\"\"\n",
    "        Build the vocabulary from the training corpus.\n",
    "        :arg texts: list of tokenized sentences.\n",
    "        \"\"\"\n",
    "        # Count the frequency of each word across all texts\n",
    "        freq = Counter([w for text in texts for w in text])\n",
    "        # Add words that meet the frequency threshold to the vocabulary\n",
    "        for w, c in freq.items():\n",
    "            if c >= self.min_freq:\n",
    "                idx = len(self.word2id)\n",
    "                self.word2id[w] = idx\n",
    "                self.id2word[idx] = w\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Convert a list of tokens into a list of integer IDs.\n",
    "        Unknown words are mapped to <UNK>.\n",
    "        \"\"\"\n",
    "        return [self.word2id.get(w, 1) for w in text]\n",
    "\n",
    "tokenizer = MiniTokenizer(min_freq=3)\n",
    "# Build vocabulary from cleaned training texts\n",
    "tokenizer.fit(cleaned_texts)\n",
    "# Check the vocabulary size\n",
    "vocab_size = len(tokenizer.word2id)\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559998cf",
   "metadata": {},
   "source": [
    "## 4.1 TECHNIQUE 2: Byte-Pair Encoding (BPE) Tokenizer\n",
    "We also build a BPE Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098f5881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size=1000):\n",
    "        \"\"\"Initialize BPE tokenizer with specified vocabulary size\"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}\n",
    "        self.merges = {}\n",
    "        \n",
    "    def get_stats(self, vocab):\n",
    "        \"\"\"Calculate frequency of adjacent symbol pairs\"\"\"\n",
    "        pairs = {}\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols)-1):\n",
    "                pair = (symbols[i], symbols[i+1])\n",
    "                pairs[pair] = pairs.get(pair, 0) + freq\n",
    "        return pairs\n",
    "    \n",
    "    def merge_vocab(self, pair, v_in):\n",
    "        \"\"\"Merge the most frequent symbol pair\"\"\"\n",
    "        v_out = {}\n",
    "        bigram = ' '.join(pair)\n",
    "        replacement = ''.join(pair)\n",
    "        for word in v_in:\n",
    "            w_out = word.replace(bigram, replacement)\n",
    "            v_out[w_out] = v_in[word]\n",
    "        return v_out\n",
    "    \n",
    "    def train(self, texts):\n",
    "        \"\"\"Train BPE tokenizer on given texts\"\"\"\n",
    "        print(\"Starting BPE tokenizer training...\")\n",
    "        \n",
    "        # Count word frequencies with character-level initialization\n",
    "        word_freq = {}\n",
    "        for text in texts:\n",
    "            for word in text:\n",
    "                # Convert word to character sequence with end token\n",
    "                tokenized_word = ' '.join(list(word)) + ' </w>'\n",
    "                word_freq[tokenized_word] = word_freq.get(tokenized_word, 0) + 1\n",
    "        \n",
    "        vocab = word_freq\n",
    "        \n",
    "        # Initialize vocabulary with special tokens and all characters\n",
    "        self.vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "        for word in vocab:\n",
    "            for char in word.split():\n",
    "                if char not in self.vocab:\n",
    "                    self.vocab[char] = len(self.vocab)\n",
    "        \n",
    "        self.merges = {}\n",
    "        num_merges = self.vocab_size - len(self.vocab)\n",
    "        \n",
    "        # BPE merging iterations\n",
    "        for i in range(num_merges):\n",
    "            pairs = self.get_stats(vocab)\n",
    "            if not pairs:\n",
    "                print(f\"No more pairs to merge. Stopped after {i} merges\")\n",
    "                break\n",
    "                \n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            best_freq = pairs[best_pair]\n",
    "            \n",
    "            if best_freq < 2:  # Stop if frequency is too low\n",
    "                print(f\"Highest pair frequency below 2. Stopped after {i} merges\")\n",
    "                break\n",
    "                \n",
    "            # Perform the merge\n",
    "            vocab = self.merge_vocab(best_pair, vocab)\n",
    "            self.merges[best_pair] = len(self.vocab)\n",
    "            merged_token = ''.join(best_pair)\n",
    "            self.vocab[merged_token] = len(self.vocab)\n",
    "            \n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Completed {i+1}/{num_merges} merges: {best_pair} -> {merged_token}\")\n",
    "        \n",
    "        print(f\"BPE training completed! Final vocabulary size: {len(self.vocab)}\")\n",
    "        print(f\"Total merges performed: {len(self.merges)}\")\n",
    "    \n",
    "    def tokenize(self, word):\n",
    "        \"\"\"Tokenize a single word using BPE\"\"\"\n",
    "        if word in self.vocab:\n",
    "            return [word]\n",
    "            \n",
    "        # Initialize as character sequence\n",
    "        tokens = list(word) + ['</w>']\n",
    "        \n",
    "        # Apply merge rules\n",
    "        while len(tokens) > 1:\n",
    "            pairs = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n",
    "            possible_merges = []\n",
    "            \n",
    "            for pair in pairs:\n",
    "                if pair in self.merges:\n",
    "                    possible_merges.append((self.merges[pair], pair))\n",
    "            \n",
    "            if not possible_merges:\n",
    "                break\n",
    "                \n",
    "            # Select the pair with smallest merge index (earliest learned)\n",
    "            _, best_pair = min(possible_merges)\n",
    "            \n",
    "            # Perform the merge\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens)-1 and (tokens[i], tokens[i+1]) == best_pair:\n",
    "                    new_tokens.append(tokens[i] + tokens[i+1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            tokens = new_tokens\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode a list of words into token IDs\"\"\"\n",
    "        token_ids = []\n",
    "        for word in text:\n",
    "            tokens = self.tokenize(word)\n",
    "            for token in tokens:\n",
    "                token_ids.append(self.vocab.get(token, self.vocab['<UNK>']))\n",
    "        return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6f467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train and Test BPE Tokenizer ---\n",
    "print(\"Training BPE tokenizer...\")\n",
    "bpe_tokenizer = BPETokenizer(vocab_size=800)\n",
    "bpe_tokenizer.train(cleaned_texts)\n",
    "\n",
    "# Test BPE tokenizer\n",
    "test_words = [\"playing\", \"unbelievable\", \"didn't\", \"happily\", \"actors\"]\n",
    "print(\"\\nBPE Tokenization Examples:\")\n",
    "for word in test_words:\n",
    "    tokens = bpe_tokenizer.tokenize(word)\n",
    "    print(f\"  '{word}' -> {tokens}\")\n",
    "\n",
    "# Test encoding functionality\n",
    "print(\"\\nBPE Encoding Examples:\")\n",
    "test_sentences = [\n",
    "    [\"I\", \"don't\", \"like\", \"this\", \"movie\"],\n",
    "    [\"The\", \"actors\", \"were\", \"excellent\"]\n",
    "]\n",
    "\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    token_ids = bpe_tokenizer.encode(sentence)\n",
    "    print(f\"  Sentence {i+1}: {sentence}\")\n",
    "    print(f\"  Encoded IDs: {token_ids}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58f826b",
   "metadata": {},
   "source": [
    "## 5. Technique Comparison and Data Saving\n",
    "\n",
    "TECHNIQUE COMPARISON: Word-Level vs BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1fa8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison testing\n",
    "test_sentences = [\n",
    "    \"I don't like this unbelievable movie\",\n",
    "    \"The actors were playing wonderfully\"\n",
    "]\n",
    "\n",
    "print(\"Tokenization Comparison Demo:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    print(f\"Sentence {i}: '{sentence}'\")\n",
    "    \n",
    "    # Word-Level tokenization\n",
    "    word_tokens = sentence.lower().split()\n",
    "    word_ids = tokenizer.encode(word_tokens)\n",
    "    print(f\"  Word-Level: {word_tokens}\")\n",
    "    \n",
    "    # BPE tokenization\n",
    "    bpe_tokens = []\n",
    "    for word in sentence.lower().split():\n",
    "        bpe_tokens.extend(bpe_tokenizer.tokenize(word))\n",
    "    print(f\"  BPE:        {bpe_tokens}\")\n",
    "    print()\n",
    "\n",
    "# Save BPE vocabulary\n",
    "print(\"Saving BPE tokenizer...\")\n",
    "bpe_out_dir = Path(\"artifacts/tokenizer_bpe\")\n",
    "bpe_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "bpe_meta = {\n",
    "    \"type\": \"bpe\",\n",
    "    \"vocab_size\": len(bpe_tokenizer.vocab),\n",
    "    \"merges_count\": len(bpe_tokenizer.merges),\n",
    "    \"vocab\": bpe_tokenizer.vocab,\n",
    "    \"merges\": list(bpe_tokenizer.merges.keys())\n",
    "}\n",
    "\n",
    "(bpe_out_dir / \"bpe_model.json\").write_text(json.dumps(bpe_meta, indent=2))\n",
    "print(f\"BPE model saved to: {bpe_out_dir / 'bpe_model.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e024dc1",
   "metadata": {},
   "source": [
    "## 6. Final Technique Comparison and Selection\n",
    "FINAL TECHNIQUE COMPARISON AND SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db778bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Metric': ['Vocabulary Size', 'OOV Handling', 'Subword Units', 'Implementation Complexity', 'Best Use Case'],\n",
    "    'Word-Level': [\n",
    "        f\"{len(tokenizer.word2id)}\", \n",
    "        'Poor (uses <UNK>)', \n",
    "        'No', \n",
    "        'Low', \n",
    "        'Baseline tasks'\n",
    "    ],\n",
    "    'BPE': [\n",
    "        f\"{len(bpe_tokenizer.vocab)}\", \n",
    "        'Good', \n",
    "        'Yes', \n",
    "        'Medium', \n",
    "        'Subword tasks'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Print comparison table\n",
    "print(f\"{'Metric':<25} | {'Word-Level':<20} | {'BPE':<20}\")\n",
    "print(\"-\" * 75)\n",
    "for i in range(len(comparison_data['Metric'])):\n",
    "    metric = comparison_data['Metric'][i]\n",
    "    word_val = comparison_data['Word-Level'][i]\n",
    "    bpe_val = comparison_data['BPE'][i]\n",
    "    print(f\"{metric:<25} | {word_val:<20} | {bpe_val:<20}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702cad39",
   "metadata": {},
   "source": [
    "## GROUP DECISION: We select Word-Level tokenizer as our final solution\n",
    "Reasoning:\n",
    "1. Word-Level tokenizer provides a solid baseline with 19,779 vocabulary size\n",
    "2. For movie review sentiment analysis, most words are common English words\n",
    "3. The simplicity of word-level tokenization allows us to focus on model architecture\n",
    "4. BPE shows excellent subword handling but adds complexity for this specific task\n",
    "5. The performance gain from BPE does not justify the added complexity for sentiment analysis\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77b231321c2d98b",
   "metadata": {},
   "source": [
    "## 7. Embedding Construction\n",
    "We assign each word a random embedding vector of dimension 50.\n",
    "Though not trained, this embedding allows us to visualize the structure of the vocabulary in a continuous space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486ab96",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99119266cdc745a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:06:59.733246Z",
     "start_time": "2025-10-13T06:06:59.720500Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set the dimensionality of the embedding vectors\n",
    "embedding_dim = 50 # Each word will be represented by a 50-dimensional vector\n",
    "# Initialize a random embedding matrix\n",
    "# Shape: (vocab_size, embedding_dim)\n",
    "# Each row corresponds to one word in the vocabulary,\n",
    "# and each column represents one \"feature\" of that word's semantic meaning.\n",
    "embedding_matrix = np.random.uniform(\n",
    "    low=-0.01, # Lower bound for random initialization\n",
    "    high=0.01, # Upper bound\n",
    "    size=(vocab_size, embedding_dim)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6bf1e062b570d0",
   "metadata": {},
   "source": [
    "## 8. Generate Sentence Embeddings and Export Data\n",
    "\n",
    "After building the tokenizer and embedding matrix, we now transform each text review into a numerical representation that can be used by other models.\n",
    "\n",
    "Steps:\n",
    "1. Encode each cleaned review into token IDs using our custom tokenizer.\n",
    "2. Pad or truncate each sequence to a fixed length (for consistency).\n",
    "3. Use the embedding matrix to map token IDs to dense vectors.\n",
    "4. Average each sentence's word embeddings into a single 50-dimensional vector.\n",
    "5. Export the resulting embeddings and labels to CSV files, so other notebooks in the group can train models using the same processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1849194e8206445a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:17:47.148462Z",
     "start_time": "2025-10-13T06:17:47.080940Z"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Step 6.1: Encode and Pad Token Sequences\n",
    "# -------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    \"\"\"\n",
    "    Pads or truncates a tokenized sentence to a fixed length.\n",
    "    Shorter sequences are padded with zeros (<PAD> = 0),\n",
    "    and longer ones are truncated.\n",
    "    \"\"\"\n",
    "    return seq[:max_len] + [0] * (max_len - len(seq))\n",
    "\n",
    "# Set a reasonable maximum length for each review\n",
    "max_len = 200\n",
    "\n",
    "# Convert each cleaned text to a sequence of token IDs\n",
    "X_encoded = [pad_sequence(tokenizer.encode(text), max_len) for text in cleaned_texts]\n",
    "\n",
    "print(\"Example encoded review:\", X_encoded[0][:20])\n",
    "print(\"Total encoded samples:\", len(X_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e53e51a6f4dc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:18:21.369280Z",
     "start_time": "2025-10-13T06:18:21.347114Z"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Step 6.2: Split Encoded Data into Train and Test Sets\n",
    "# -------------------------------------------------\n",
    "\n",
    "# 80% training, 20% testing\n",
    "train_size = int(0.8 * len(X_encoded))\n",
    "\n",
    "# Convert lists to numpy arrays for easier handling\n",
    "X_train_seq = np.array(X_encoded[:train_size])\n",
    "X_test_seq = np.array(X_encoded[train_size:])\n",
    "\n",
    "# Convert labels (\"pos\"/\"neg\") into binary form (1/0)\n",
    "y_train = np.array([1 if label == 'pos' else 0 for _, label in documents[:train_size]])\n",
    "y_test = np.array([1 if label == 'pos' else 0 for _, label in documents[train_size:]])\n",
    "\n",
    "print(\"Train set shape:\", X_train_seq.shape)\n",
    "print(\"Test set shape:\", X_test_seq.shape)\n",
    "print(\"Example label distribution:\", np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a24d1a8dbd0258",
   "metadata": {},
   "source": [
    "## 9. Inspect Vocabulary\n",
    "Let's look at a few words and their corresponding IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c53397a3af31d5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:07:01.784529Z",
     "start_time": "2025-10-13T06:07:01.779417Z"
    }
   },
   "outputs": [],
   "source": [
    "print(list(tokenizer.word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f33bfed2a913d",
   "metadata": {},
   "source": [
    "## 10. Visualize Word Embeddings with PCA & t-SNE\n",
    "We use PCA to project the 50-dimensional embedding vectors into 2D for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c739203ac232ce4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:10:28.747360Z",
     "start_time": "2025-10-13T06:10:28.742763Z"
    }
   },
   "outputs": [],
   "source": [
    "# To avoid overcrowded plots, we randomly pick 300 words from the vocabulary.\n",
    "subset_size = 300\n",
    "subset_ids = random.sample(range(2, vocab_size), subset_size)  # skip <PAD> and <UNK>\n",
    "subset_words = [tokenizer.id2word[i] for i in subset_ids]\n",
    "subset_vectors = embedding_matrix[subset_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7660908c7e0fb40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:10:31.442144Z",
     "start_time": "2025-10-13T06:10:31.336884Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(subset_vectors)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], s=8, alpha=0.6, color='steelblue')\n",
    "for i, word in enumerate(subset_words[:80]):  # label only a few to keep it readable\n",
    "    plt.text(pca_result[i, 0] + 0.05, pca_result[i, 1] + 0.05, word, fontsize=8)\n",
    "plt.title(\"PCA Visualization of Word Embeddings\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268891818064dd7c",
   "metadata": {},
   "source": [
    "t-SNE can better capture local clusters of similar embeddings.\n",
    "We visualize a random subset of the vocabulary to maintain clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2fff51ff50d17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:11:44.985161Z",
     "start_time": "2025-10-13T06:11:44.410464Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# t-SNE often reveals more local structure (clusters),\n",
    "# but is slower and non-linear.\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=200.0, init='pca', random_state=42)\n",
    "import numpy as np\n",
    "subset_vectors = np.asarray(subset_vectors, dtype=np.float64)\n",
    "tsne_result = tsne.fit_transform(subset_vectors)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(tsne_result[:, 0], tsne_result[:, 1], s=8, alpha=0.6, color='darkorange')\n",
    "for i, word in enumerate(subset_words[:80]):  # label a few words\n",
    "    plt.text(tsne_result[i, 0] + 0.05, tsne_result[i, 1] + 0.05, word, fontsize=8)\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1dee5ef4e87dd0",
   "metadata": {},
   "source": [
    "## 11. Save Embedded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896698702fd899a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T06:20:14.081231Z",
     "start_time": "2025-10-13T06:20:14.022777Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert numpy arrays into DataFrames\n",
    "train_df = pd.DataFrame(X_train_seq)\n",
    "train_df['label'] = y_train  # add target column at the end\n",
    "\n",
    "test_df = pd.DataFrame(X_test_seq)\n",
    "test_df['label'] = y_test\n",
    "\n",
    "# Save as CSV files\n",
    "train_df.to_csv(\"dataset/train_embeddings.csv\", index=False)\n",
    "test_df.to_csv(\"dataset/test_embeddings.csv\", index=False)\n",
    "\n",
    "print(\"Saved to train_embeddings.csv and test_embeddings.csv\")\n",
    "print(\"Train shape:\", train_df.shape, \"Test shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54ca3a8",
   "metadata": {},
   "source": [
    "## 12) Export vocabulary for interactive inference\n",
    "Finally, we exports a minimal vocabulary file so the model notebook can map raw text to token IDs.\n",
    "Output: `artifacts/tokenizer_word/tokenizer.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccf3fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export tokenizer vocabulary for Model notebook (no third-party libs)\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "assert 'tokenizer' in globals(), \"Tokenizer not found. Make sure you trained it (fit) above.\"\n",
    "\n",
    "out_dir = Path(\"artifacts/tokenizer_word\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Build id2word list from tokenizer, supporting both list and dict implementations\n",
    "id2word = getattr(tokenizer, \"id2word\", None)\n",
    "id2word_list = None\n",
    "\n",
    "if isinstance(id2word, list):\n",
    "    id2word_list = list(id2word)\n",
    "elif isinstance(id2word, dict):\n",
    "    id2word_list = [w for i, w in sorted(id2word.items(), key=lambda x: x[0])]\n",
    "else:\n",
    "    # fallback: reconstruct from word2id if possible\n",
    "    w2i = getattr(tokenizer, \"word2id\", None)\n",
    "    if isinstance(w2i, dict):\n",
    "        id_max = max(w2i.values()) if w2i else -1\n",
    "        id2word_list = [\"\"] * (id_max + 1)\n",
    "        for w, i in w2i.items():\n",
    "            id2word_list[i] = w\n",
    "\n",
    "assert id2word_list and isinstance(id2word_list, list), \"Failed to obtain tokenizer vocabulary.\"\n",
    "\n",
    "meta = {\n",
    "    \"type\": \"word\",\n",
    "    \"id2word\": id2word_list\n",
    "}\n",
    "(out_dir / \"tokenizer.json\").write_text(json.dumps(meta, ensure_ascii=False, indent=2))\n",
    "print(\"Saved vocabulary to\", out_dir / \"tokenizer.json\")\n",
    "print(\"Vocab size:\", len(id2word_list), \"First 10:\", id2word_list[:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
