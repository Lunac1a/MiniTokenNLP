{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22a06f24",
   "metadata": {},
   "source": [
    "# Model.ipynb — Baseline Text Classifier (Standard Library Only)\n",
    "\n",
    "**Goal:** Train and evaluate a simple Multinomial Naive Bayes classifier using the token-ID sequences\n",
    "exported by `1_Tokenizer & Embedding.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b71957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and config (standard library only) ===\n",
    "import csv, math, json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# Input files produced by the first notebook\n",
    "TRAIN_CSV = Path(\"dataset/train_embeddings.csv\")\n",
    "TEST_CSV  = Path(\"dataset/test_embeddings.csv\")\n",
    "\n",
    "# <PAD> token id. Must match the first notebook.\n",
    "PAD_ID = 0\n",
    "\n",
    "# Basic checks\n",
    "assert TRAIN_CSV.exists(), f\"Missing {TRAIN_CSV}. Please export it from the first notebook.\"\n",
    "assert TEST_CSV.exists(),  f\"Missing {TEST_CSV}. Please export it from the first notebook.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4595d1",
   "metadata": {},
   "source": [
    "## 1. Load sequences from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e4e3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1600  Test size: 400\n",
      "Example train row (first 15 ids): [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "def load_xy_from_csv(path: Path, max_len: int = 200):\n",
    "    \"\"\"Load a CSV where the first `max_len` columns are token ids and the last column is `label`.\n",
    "    Returns: X (list[list[int]]), y (list[int]).\"\"\"\n",
    "    X, y = [], []\n",
    "    with open(path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader, None)  # skip header if present\n",
    "        for row in reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            try:\n",
    "                ids = [int(x) for x in row[:max_len]]\n",
    "                label = int(row[max_len])\n",
    "            except Exception:\n",
    "                # skip malformed lines\n",
    "                continue\n",
    "            X.append(ids)\n",
    "            y.append(label)\n",
    "    return X, y\n",
    "\n",
    "X_train_seq, y_train = load_xy_from_csv(TRAIN_CSV, max_len=200)\n",
    "X_test_seq,  y_test  = load_xy_from_csv(TEST_CSV,  max_len=200)\n",
    "\n",
    "print(\"Train size:\", len(X_train_seq), \" Test size:\", len(X_test_seq))\n",
    "print(\"Example train row (first 15 ids):\", X_train_seq[0][:15] if X_train_seq else [])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d74196a",
   "metadata": {},
   "source": [
    "## 2. Convert sequences to sparse Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcd05d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_counts(seq, pad_id: int = PAD_ID):\n",
    "    \"\"\"Convert a fixed-length token-id sequence into a sparse bag-of-words dict.\n",
    "    - Skip the PAD token (pad_id).\n",
    "    - Return a dict: token_id -> count.\"\"\"\n",
    "    bag = defaultdict(int)\n",
    "    for tid in seq:\n",
    "        if tid != pad_id:\n",
    "            bag[tid] += 1\n",
    "    return bag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afb695f",
   "metadata": {},
   "source": [
    "## 3. Train Multinomial Naive Bayes (self-implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e371145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained. Classes: [0, 1] Vocab size: 18602\n"
     ]
    }
   ],
   "source": [
    "def train_mnb(X_seqs, y, alpha: float = 1.0, pad_id: int = PAD_ID):\n",
    "    \"\"\"Train a Multinomial Naive Bayes model:\n",
    "    - Estimate class priors P(c)\n",
    "    - Estimate conditional probabilities P(token_id | c) with Laplace smoothing\n",
    "    - Store everything in log space (log_prior, log_likelihood)\n",
    "\n",
    "    Returns: a model dict.\"\"\"\n",
    "    class_count = defaultdict(int)                               # number of docs per class\n",
    "    token_count_per_class = defaultdict(lambda: defaultdict(int)) # token counts per class\n",
    "    vocab = set()                                                 # set of observed token ids\n",
    "\n",
    "    # Count statistics\n",
    "    for seq, label in zip(X_seqs, y):\n",
    "        class_count[label] += 1\n",
    "        bag = seq_to_counts(seq, pad_id=pad_id)\n",
    "        for tid, c in bag.items():\n",
    "            token_count_per_class[label][tid] += c\n",
    "            vocab.add(tid)\n",
    "\n",
    "    # log P(c)\n",
    "    total_docs = sum(class_count.values())\n",
    "    log_prior = {c: math.log(class_count[c] / total_docs) for c in class_count}\n",
    "\n",
    "    # log P(tid | c)\n",
    "    V = len(vocab)  # vocabulary size based on observed token ids\n",
    "    log_likelihood = {c: {} for c in class_count}\n",
    "    for c in class_count:\n",
    "        total_tokens_c = sum(token_count_per_class[c].values())\n",
    "        denom = total_tokens_c + alpha * V\n",
    "        for tid in vocab:\n",
    "            num = token_count_per_class[c].get(tid, 0) + alpha\n",
    "            log_likelihood[c][tid] = math.log(num / denom)\n",
    "\n",
    "    return {\n",
    "        \"log_prior\": log_prior,\n",
    "        \"log_likelihood\": log_likelihood,\n",
    "        \"vocab\": vocab,\n",
    "        \"alpha\": alpha,\n",
    "        \"pad_id\": pad_id,\n",
    "    }\n",
    "\n",
    "model = train_mnb(X_train_seq, y_train, alpha=1.0, pad_id=PAD_ID)\n",
    "print(\"Model trained. Classes:\", list(model[\"log_prior\"].keys()), \"Vocab size:\", len(model[\"vocab\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4313c256",
   "metadata": {},
   "source": [
    "## 4. Predict and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "650b39ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7875\n",
      "F1 (positive=1): 0.7769\n"
     ]
    }
   ],
   "source": [
    "def predict_one_mnb(model, seq):\n",
    "    \"\"\"Predict the class (0/1) for a single sequence using the trained MNB model.\n",
    "    Sparse scoring: only sum over tokens that actually appear in the sequence.\"\"\"\n",
    "    bag = seq_to_counts(seq, pad_id=model[\"pad_id\"])\n",
    "    best_c, best_score = None, None\n",
    "    for c in model[\"log_prior\"]:\n",
    "        score = model[\"log_prior\"][c]\n",
    "        for tid, cnt in bag.items():\n",
    "            if tid in model[\"vocab\"]:\n",
    "                score += cnt * model[\"log_likelihood\"][c].get(tid, 0.0)\n",
    "        if best_score is None or score > best_score:\n",
    "            best_c, best_score = c, score\n",
    "    return best_c\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct = sum(int(a == b) for a, b in zip(y_true, y_pred))\n",
    "    return correct / max(1, len(y_true))\n",
    "\n",
    "def f1_binary(y_true, y_pred, positive=1):\n",
    "    \"\"\"Simple F1 for binary classification.\"\"\"\n",
    "    tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == positive and yp == positive)\n",
    "    fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt != positive and yp == positive)\n",
    "    fn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == positive and yp != positive)\n",
    "    prec = tp / max(1, tp + fp)\n",
    "    rec  = tp / max(1, tp + fn)\n",
    "    return 0.0 if (prec + rec) == 0 else 2 * prec * rec / (prec + rec)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = [predict_one_mnb(model, seq) for seq in X_test_seq]\n",
    "print(\"Accuracy:\", round(accuracy(y_test, y_pred), 4))\n",
    "print(\"F1 (positive=1):\", round(f1_binary(y_test, y_pred, positive=1), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9291a366",
   "metadata": {},
   "source": [
    "## 5. Prediction helper: predict from raw token-id sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f12e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_from_ids demo (0/1): 1\n"
     ]
    }
   ],
   "source": [
    "def predict_from_ids(seq_ids, max_len: int = 200):\n",
    "    \"\"\"\n",
    "    Predict from a raw token-id sequence of arbitrary length.\n",
    "\n",
    "    Steps:\n",
    "      1. If the sequence is longer than `max_len`, it is truncated.\n",
    "      2. If the sequence is shorter than `max_len`, it is padded\n",
    "         with <PAD> tokens (ID defined by PAD_ID, usually 0).\n",
    "      3. The fixed-length sequence is passed to `predict_one_mnb`\n",
    "         for classification.\n",
    "\n",
    "    Args:\n",
    "        seq_ids (list[int]): token IDs of the sentence or text.\n",
    "        max_len (int): desired fixed sequence length (default 200).\n",
    "\n",
    "    Returns:\n",
    "        int: predicted label (0 = negative, 1 = positive).\n",
    "    \"\"\"\n",
    "    # Case 1: sequence longer than 200 → keep only first 200 tokens\n",
    "    if len(seq_ids) >= max_len:\n",
    "        seq = seq_ids[:max_len]\n",
    "    else:\n",
    "        # Case 2: sequence shorter than 200 → pad with PAD_ID (zeros)\n",
    "        seq = seq_ids + [PAD_ID] * (max_len - len(seq_ids))\n",
    "\n",
    "    # Send the processed sequence to the trained Naive Bayes model\n",
    "    return predict_one_mnb(model, seq)\n",
    "\n",
    "# Demo of predict_from_ids\n",
    "print(\"predict_from_ids demo (0/1):\", predict_from_ids(X_test_seq[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92105527",
   "metadata": {},
   "source": [
    "## 6. Interactive single-sentence prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd79c969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a sentence to classify (blank line to exit):\n",
      "Input: i love this movieee\n",
      "Prediction: positive\n",
      "Input: this movie so boring\n",
      "Prediction: negative\n"
     ]
    }
   ],
   "source": [
    "# Type any raw sentence and get the predicted label.\n",
    "# Requirements:\n",
    "#  - The model in this notebook must already be trained (run previous cells).\n",
    "#  - A tokenizer vocabulary must exist at artifacts/tokenizer_word/tokenizer.json\n",
    "#    (export it once from the tokenizer notebook).\n",
    "\n",
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "TOKENIZER_JSON = Path(\"artifacts/tokenizer_word/tokenizer.json\")\n",
    "\n",
    "def _basic_normalize(text: str) -> str:\n",
    "    \"\"\"Lowercase + keep letters/digits/apostrophes + collapse spaces.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s']\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def _load_tokenizer_vocab(json_path=TOKENIZER_JSON):\n",
    "    if not json_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing {json_path}. Please export tokenizer_word/tokenizer.json in the tokenizer notebook.\"\n",
    "        )\n",
    "    meta = json.loads(json_path.read_text())\n",
    "    id2word = meta.get(\"id2word\", [])\n",
    "    word2id = {w: i for i, w in enumerate(id2word)}\n",
    "    ttype = meta.get(\"type\", \"word\")\n",
    "    return {\"type\": ttype, \"id2word\": id2word, \"word2id\": word2id}\n",
    "\n",
    "def _encode_with_vocab(text: str, vocab: dict):\n",
    "    \"\"\"Minimal encoder for raw text using the saved vocabulary (UNK=1).\"\"\"\n",
    "    s = _basic_normalize(text)\n",
    "    toks = list(s) if vocab[\"type\"] == \"char\" else s.split()\n",
    "    ids = [vocab[\"word2id\"].get(t, 1) for t in toks]  # UNK=1\n",
    "    return ids\n",
    "\n",
    "def predict_one_text(text: str):\n",
    "    \"\"\"Raw text -> ids -> pad -> predict via the trained MNB model.\"\"\"\n",
    "    vocab = _load_tokenizer_vocab()\n",
    "    ids = _encode_with_vocab(text, vocab)\n",
    "    return predict_from_ids(ids)  # uses padding and calls predict_one_mnb(model, ...)\n",
    "\n",
    "print(\"Type a sentence to classify (blank line to exit):\")\n",
    "while True:\n",
    "    try:\n",
    "        s = input(\"> \").strip()\n",
    "    except EOFError:\n",
    "        break\n",
    "    if not s:\n",
    "        break\n",
    "    label = predict_one_text(s)\n",
    "    print(f\"Input: {s}\")\n",
    "    print(\"Prediction:\", \"positive\" if label == 1 else \"negative\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
